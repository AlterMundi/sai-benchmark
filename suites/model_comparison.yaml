name: "Model Comparison Benchmark"
description: "Compare performance across different vision-language models for fire detection"
version: "1.0"

# Single prompt for fair comparison
prompts:
  - "early_fire_json"

# Multiple models to compare
models:
  - "qwen2.5-vl:7b"
  - "qwen2.5-vl-7b-hf"
  - "gpt-4o"
  - "llava:13b"
  - "minicpm-v:8b"

# Standard dataset
datasets:
  - "data/fire_benchmark_standard"

# Core metrics for comparison
metrics:
  - "accuracy"
  - "precision"
  - "recall"
  - "f1_score"
  - "latency"
  - "cost"
  - "bbox_iou"
  - "parse_success_rate"

# Conservative engine settings
engine_config:
  timeout: 180
  max_retries: 3
  temperature: 0.0  # Deterministic for comparison

# Test configuration
test_config:
  max_images_per_sequence: 3  # Faster execution
  iou_threshold: 0.5
  confidence_threshold: 0.7